{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Install Libs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T07:50:23.134732Z",
     "iopub.status.busy": "2022-06-01T07:50:23.133326Z",
     "iopub.status.idle": "2022-06-01T07:51:24.378933Z",
     "shell.execute_reply": "2022-06-01T07:51:24.377817Z",
     "shell.execute_reply.started": "2022-06-01T07:50:23.134682Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n",
    "!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T07:51:24.384621Z",
     "iopub.status.busy": "2022-06-01T07:51:24.383693Z",
     "iopub.status.idle": "2022-06-01T07:51:26.951788Z",
     "shell.execute_reply": "2022-06-01T07:51:26.950793Z",
     "shell.execute_reply.started": "2022-06-01T07:51:24.38455Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "import kornia.feature.loftr as loftr\n",
    "\n",
    "import gc\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "def sift_image(im0):\n",
    "    '''Load and format image for SIFT'''\n",
    "    im0 = cv2.imread(im0)\n",
    "    scale = 840 / max(im0.shape[0], im0.shape[1]) \n",
    "    w = int(im0.shape[1] * scale)\n",
    "    h = int(im0.shape[0] * scale)\n",
    "    im0 = cv2.resize(im0, (w, h))\n",
    "    return im0\n",
    "\n",
    "def ExtractSiftFeatures(image, detector, num_features):\n",
    "    '''Compute SIFT features for a given image.'''\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return detector.detectAndCompute(gray, None)[:num_features]\n",
    "\n",
    "def ArrayFromCvKps(kps):\n",
    "    '''Convenience function to convert OpenCV keypoints into a simple numpy array.'''\n",
    "    return np.array([kp.pt for kp in kps])\n",
    "\n",
    "def FlattenMatrix(M, num_digits=8):\n",
    "    '''Convenience function to write CSV files.'''\n",
    "    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n",
    "\n",
    "\n",
    "def fast_cleanup(dp1,dp2,sf1,sf2, device):\n",
    "    '''\n",
    "    Sorting function that returns keypoints that are close \n",
    "    matches between LoFTr and SIFT.\n",
    "    '''\n",
    "    tmp1 = []\n",
    "    tmp2 = []\n",
    "    \n",
    "    tdp1 = torch.Tensor(dp1).to(device)\n",
    "    tdp2 = torch.Tensor(dp2)\n",
    "    tsf1 = torch.Tensor(sf1)\n",
    "\n",
    "    x = torch.stack([tsf1]*tdp1.shape[0]).to(device)\n",
    "    rr = (torch.linalg.norm(x-tdp1[:, None], dim=2) <= 1).nonzero()\n",
    "    tmp1 = tdp1[rr[:,0]].cpu().numpy()\n",
    "    tmp2 = tdp2[rr[:,0]].cpu().numpy()\n",
    "    \n",
    "    return tmp1, tmp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T07:51:27.260506Z",
     "iopub.status.busy": "2022-06-01T07:51:27.259767Z",
     "iopub.status.idle": "2022-06-01T07:51:27.281123Z",
     "shell.execute_reply": "2022-06-01T07:51:27.28008Z",
     "shell.execute_reply.started": "2022-06-01T07:51:27.260363Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "src = '/kaggle/input/image-matching-challenge-2022/'\n",
    "\n",
    "test_samples = []\n",
    "with open(f'{src}/test.csv') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    for i, row in enumerate(reader):\n",
    "        # Skip header.\n",
    "        if i == 0:\n",
    "            continue\n",
    "        test_samples += [row]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Inference***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T07:58:26.502562Z",
     "iopub.status.busy": "2022-06-01T07:58:26.502243Z",
     "iopub.status.idle": "2022-06-01T07:58:26.91963Z",
     "shell.execute_reply": "2022-06-01T07:58:26.918665Z",
     "shell.execute_reply.started": "2022-06-01T07:58:26.50253Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features = 8000\n",
    "\n",
    "mech = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(mech)\n",
    "\n",
    "matcher = loftr.LoFTR(pretrained=None)\n",
    "matcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])\n",
    "matcher = matcher.to(device).eval()\n",
    "\n",
    "detector = cv2.SIFT_create(num_features, contrastThreshold=-10000, edgeThreshold=-10000)\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T08:16:16.416227Z",
     "iopub.status.busy": "2022-06-01T08:16:16.415901Z",
     "iopub.status.idle": "2022-06-01T08:16:30.294481Z",
     "shell.execute_reply": "2022-06-01T08:16:30.29345Z",
     "shell.execute_reply.started": "2022-06-01T08:16:16.416196Z"
    }
   },
   "outputs": [],
   "source": [
    "F_dict = {}\n",
    "import time\n",
    "\n",
    "lesser = False\n",
    "\n",
    "for i, row in enumerate(test_samples): \n",
    "    sample_id, batch_id, image_1_id, image_2_id = row \n",
    "\n",
    "    ## load image pairs ##\n",
    "    image_1 = sift_image(f'{src}/test_images/{batch_id}/{image_1_id}.png')\n",
    "    image_2 = sift_image(f'{src}/test_images/{batch_id}/{image_2_id}.png')\n",
    "\n",
    "    ## SIFT ##\n",
    "    keypoints_1, descriptors_1 = ExtractSiftFeatures(image_1, detector, num_features)\n",
    "    keypoints_2, descriptors_2 = ExtractSiftFeatures(image_2, detector, num_features)\n",
    "\n",
    "    ## LoFTr ##\n",
    "    timage_1 = K.color.bgr_to_rgb(K.image_to_tensor(image_1, False).float() /255.).to(device)\n",
    "    timage_2 = K.color.bgr_to_rgb(K.image_to_tensor(image_2, False).float() /255.).to(device)\n",
    "    input_dict = {\"image0\": K.color.rgb_to_grayscale(timage_1), \"image1\": K.color.rgb_to_grayscale(timage_2)}\n",
    "    with torch.no_grad():\n",
    "        correspondences = matcher(input_dict)\n",
    "\n",
    "    ## load matching points to cpu ##\n",
    "    mkpts1 = correspondences['keypoints0'].cpu().numpy()\n",
    "    mkpts2 = correspondences['keypoints1'].cpu().numpy()\n",
    "\n",
    "    ### Brute-Force Matching ###\n",
    "    cv_matches = bf.match(descriptors_1, descriptors_2)\n",
    "    \n",
    "    matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n",
    "    cur_kp_1 = ArrayFromCvKps(keypoints_1)\n",
    "    cur_kp_2 = ArrayFromCvKps(keypoints_2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Need 8 or more points to reconstruct Fundamental Matrix ##\n",
    "    if len(mkpts1) > 7:\n",
    "        if len(cur_kp_1[matches[:, 0]]) > 7:\n",
    "            '''Both LoFTr and SIFT have enough matches'''\n",
    "            f1, f2 = fast_cleanup(mkpts1, mkpts2, cur_kp_1[matches[:, 0]], cur_kp_2[matches[:, 1]], device)\n",
    "            \n",
    "            if len(f1) > 7:\n",
    "                '''More than 7 strong matches between LoFTr and SIFT'''\n",
    "                F, inliers = cv2.findFundamentalMat(f1, f2, cv2.USAC_MAGSAC, 0.1845, 0.999999, 220000)\n",
    "                inliers = inliers > 0\n",
    "                assert F.shape == (3, 3), 'Malformed F?'\n",
    "                F_dict[sample_id] = F\n",
    "                lesser = 1\n",
    "            else:\n",
    "                '''Default to LoFTr if sorting has less than 8 matches'''\n",
    "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.1845, 0.999999, 220000)\n",
    "                inliers = inliers > 0\n",
    "                assert F.shape == (3, 3), 'Malformed F?'\n",
    "                F_dict[sample_id] = F\n",
    "                lesser = 0\n",
    "        \n",
    "        else:\n",
    "            '''Default to LoFTr if SIFT has less than 8 matches'''\n",
    "            F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.1845, 0.999999, 220000)\n",
    "            inliers = inliers > 0\n",
    "            assert F.shape == (3, 3), 'Malformed F?'\n",
    "            F_dict[sample_id] = F\n",
    "            lesser = 0\n",
    "            \n",
    "    else:    \n",
    "        if len(cur_kp_1[matches[:, 0]]) > 7:\n",
    "            '''Default to SIFT if LoFTr has less than 8 matches'''\n",
    "            F, inliers = cv2.findFundamentalMat(cur_kp_1[matches[:, 0]], cur_kp_2[matches[:, 1]], cv2.USAC_MAGSAC, 0.1845, 0.999999, 220000)\n",
    "            inliers = inliers > 0\n",
    "            assert F.shape == (3, 3), 'Malformed F?'\n",
    "            F_dict[sample_id] = F\n",
    "            lesser = 0\n",
    "            \n",
    "        else:\n",
    "            '''Zero matrix if both SIFT and LoFTr have less than 8 matches'''\n",
    "            F_dict[sample_id] = torch.zeros((3,3))\n",
    "            continue\n",
    "            \n",
    "        \n",
    "\n",
    "    ## Draw keypoint matches across both Image1 and Image2 ##\n",
    "    if (i < 3):\n",
    "        if lesser == 0:\n",
    "            inpt1, inpt2 = mkpts1, mkpts2\n",
    "        elif lesser == 1:\n",
    "            inpt1, inpt2 = f1, f2\n",
    "        elif lesser == 2:\n",
    "            inpt1, inpt2 = cur_kp_1[matches[:, 0]], cur_kp_2[matches[:, 1]]\n",
    "            \n",
    "        \n",
    "        draw_LAF_matches(KF.laf_from_center_scale_ori(torch.from_numpy(inpt1).view(1,-1, 2),\n",
    "                                    torch.ones(inpt1.shape[0]).view(1,-1, 1, 1),\n",
    "                                    torch.ones(inpt1.shape[0]).view(1,-1, 1)),\n",
    "\n",
    "        KF.laf_from_center_scale_ori(torch.from_numpy(inpt2).view(1,-1, 2),\n",
    "                                    torch.ones(inpt2.shape[0]).view(1,-1, 1, 1),\n",
    "                                    torch.ones(inpt2.shape[0]).view(1,-1, 1)),\n",
    "        \n",
    "        torch.arange(inpt1.shape[0]).view(-1,1).repeat(1,2),\n",
    "        image_1,\n",
    "        image_2,\n",
    "        inliers,\n",
    "        draw_dict={'inlier_color': (0.2, 1, 0.2),\n",
    "                   'tentative_color': None, \n",
    "                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n",
    "    \n",
    "    \n",
    "## write fundamental matrix into csv ##\n",
    "try:\n",
    "    with open('submission.csv', 'w') as f:\n",
    "        f.write('sample_id,fundamental_matrix\\n')\n",
    "        for sample_id, F in F_dict.items():\n",
    "            f.write(f'{sample_id},{FlattenMatrix(F)}\\n')\n",
    "except:\n",
    "    #check if error is here\n",
    "    pd.DataFrame().to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
